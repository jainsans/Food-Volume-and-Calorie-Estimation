{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4681bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a776429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training, validation, and testing data directories\n",
    "train_dir = r\"C:\\Users\\hp\\Desktop\\ResNet 101\\train\"\n",
    "val_dir = r\"C:\\Users\\hp\\Desktop\\ResNet 101\\val\"\n",
    "test_dir = r\"C:\\Users\\hp\\Desktop\\ResNet 101\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "283cf030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input image size\n",
    "img_size = 224\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ece0d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 698 images belonging to 14 classes.\n",
      "Found 140 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define data augmentation parameters for the training dataset\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,  # rescale pixel values to [0,1]\n",
    "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        shear_range=0.2,  # randomly apply shearing transformations\n",
    "        zoom_range=0.2,  # randomly zoom in and out\n",
    "        horizontal_flip=True)  # randomly flip images horizontally\n",
    "\n",
    "# Define data augmentation parameters for the validation dataset\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)  # just rescale pixel values\n",
    "\n",
    "# Load and prepare your training data\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "        r\"C:\\Users\\hp\\Desktop\\ResNet 101\\train\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Load and prepare your validation data\n",
    "val_data = val_datagen.flow_from_directory(\n",
    "        r\"C:\\Users\\hp\\Desktop\\ResNet 101\\val\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d70199e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 698 images belonging to 14 classes.\n",
      "Found 140 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set up the data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "243e93e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the md5 file hash does not match the original value of a268eb855778b3df3c7506639542a6af so we will re-download the data.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 11s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the ResNet101 model and remove the top layers\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb9f325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2838939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new layers for classification\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(524, activation='relu')(x)\n",
    "x = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d20d78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5277d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb2c912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Adam optimizer\n",
    "adam_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "401de7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1410c21b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Could not interpret optimizer identifier:', <class 'tensorflow.python.keras.optimizers.Adagrad'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-d6ea4f4aa794>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Compile the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m       \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mrun_eagerly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run_eagerly'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# Validate that arguments passed by the user to `compile` are supported by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Could not interpret optimizer identifier:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: ('Could not interpret optimizer identifier:', <class 'tensorflow.python.keras.optimizers.Adagrad'>)"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5784cfb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 11s 2s/step - loss: 14.9428 - acc: 0.0714\n",
      "22/22 [==============================] - 133s 6s/step - loss: 14.9958 - acc: 0.0702 - val_loss: 14.9428 - val_acc: 0.0714\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 11s 2s/step - loss: 14.7749 - acc: 0.0714\n",
      "22/22 [==============================] - 125s 6s/step - loss: 14.9857 - acc: 0.0702 - val_loss: 14.7749 - val_acc: 0.0714\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 15.1107 - acc: 0.0714\n",
      "22/22 [==============================] - 124s 6s/step - loss: 14.9807 - acc: 0.0702 - val_loss: 15.1107 - val_acc: 0.0714\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 12s 2s/step - loss: 14.7749 - acc: 0.0714\n",
      "22/22 [==============================] - 127s 6s/step - loss: 14.9857 - acc: 0.0702 - val_loss: 14.7749 - val_acc: 0.0714\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 13s 3s/step - loss: 14.9428 - acc: 0.0714\n",
      "22/22 [==============================] - 153s 7s/step - loss: 14.9958 - acc: 0.0702 - val_loss: 14.9428 - val_acc: 0.0714\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 11s 2s/step - loss: 15.1107 - acc: 0.0714\n",
      "22/22 [==============================] - 142s 6s/step - loss: 14.9807 - acc: 0.0702 - val_loss: 15.1107 - val_acc: 0.0714\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 12s 2s/step - loss: 14.9428 - acc: 0.0714\n",
      "22/22 [==============================] - 127s 6s/step - loss: 14.9807 - acc: 0.0702 - val_loss: 14.9428 - val_acc: 0.0714\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 12s 2s/step - loss: 14.9428 - acc: 0.0714\n",
      "22/22 [==============================] - 139s 6s/step - loss: 14.9908 - acc: 0.0702 - val_loss: 14.9428 - val_acc: 0.0714\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 14s 3s/step - loss: 14.9428 - acc: 0.0714\n",
      "22/22 [==============================] - 156s 7s/step - loss: 14.9857 - acc: 0.0702 - val_loss: 14.9428 - val_acc: 0.0714\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 14.7749 - acc: 0.0714\n",
      "22/22 [==============================] - 170s 8s/step - loss: 14.9807 - acc: 0.0702 - val_loss: 14.7749 - val_acc: 0.0714\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.n // val_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "400bd524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 147 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf551b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 12s 2s/step - loss: 14.8403 - acc: 0.0816\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8a49773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.08163265\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10cad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
